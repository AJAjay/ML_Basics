{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=10000, n_classes=2, weights=[0.8,0.2], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "model = LogisticRegression()\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "\n",
    "# # keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy \n",
    "print (\"Accuracy of the model: \", accuracy_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y)[0].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(testy, yhat), index=['Negative Class', 'Positive Class'], \n",
    "                          columns=['Negative Class', 'Positive Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Actual Split: \", np.bincount(testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Accuracy is a Trap??\n",
    "\n",
    "![title](img/trap.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "![title](img/confusion.png)\n",
    "\n",
    "------------\n",
    "\n",
    "## Type 1 and Type 2 Error\n",
    "\n",
    "![title](img/error.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contigency = pd.DataFrame(confusion_matrix(testy, yhat))\n",
    "contigency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = contigency.iloc[0,0]\n",
    "fp = contigency.iloc[0,1]\n",
    "fn = contigency.iloc[1,0]\n",
    "tp = contigency.iloc[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Positive Rate/Sensitivity/Recall - proportion of actual positives which are predicted positive\n",
    "\n",
    "###### Effectiveness of a classifier to identify positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp/ (tp+fn)\n",
    "print(tpr)\n",
    "\n",
    "recall = tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Rate/False Alarm Rate\n",
    "\n",
    "##### It summarizes how often a positive class is predicted when the actual outcome is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp/ (tn+fp)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision - proportion of predicted positives which are actual positive\n",
    "\n",
    "##### It describes how good a model is at predicting the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (fp + tp)\n",
    "print(\"precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score - Harmonic Mean of Precision and Recall\n",
    "\n",
    "![title](img/harmonic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2 / (1/recall + 1/precision)\n",
    "print (\"F1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification report \n",
    "print (\"Classification Report : \\n\\n\", classification_report(testy, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve  - Receiver operating characteristic '\n",
    "\n",
    "----\n",
    "It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate.\n",
    "\n",
    "![title](img/roc1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=10000, n_classes=2, weights=[0.5,0.5], random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "# fit a model\n",
    "model = LogisticRegression()\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class values\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy \n",
    "\n",
    "print (\"Accuracy of the model: \", accuracy_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "auc = roc_auc_score(testy, probs)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(testy, probs)\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fpr)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Guess example - Diagonal Line\n",
    "y_rand_prob = np.random.rand(5000,)\n",
    "np.bincount((y_rand_prob >=0.5).astype(np.int64))\n",
    "y_rand = np.random.randint(0,2, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "auc = roc_auc_score(testy, y_rand_prob)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(testy, y_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(testy, y_rand_prob)\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Space - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following models:\n",
    "![title](img/ROC_explained.png)    \n",
    "\n",
    "\n",
    "* PPV - Positive Predicted Value or Precision\n",
    "\n",
    "-----------------\n",
    "\n",
    "![title](img/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision Recall Curve --- \n",
    "# precision-recall curve and f1\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[1,1], random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
    "# calculate F1 score\n",
    "f1 = f1_score(testy, yhat)\n",
    "# calculate precision-recall AUC\n",
    "auc = auc(recall, precision)\n",
    "# calculate average precision score\n",
    "ap = average_precision_score(testy, probs)\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n",
    "# plot no skill\n",
    "pyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(recall, precision, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If Imbalanced dataset then Precision recall curve is more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roc curve and auc on imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9,0.09], random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(testy, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(testy, probs)\n",
    "# plot no skill\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9,0.09], random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "# calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(testy, probs)\n",
    "# calculate F1 score\n",
    "f1 = f1_score(testy, yhat)\n",
    "# calculate precision-recall AUC\n",
    "auc = auc(recall, precision)\n",
    "# calculate average precision score\n",
    "ap = average_precision_score(testy, probs)\n",
    "print('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n",
    "# plot no skill\n",
    "pyplot.plot([0, 1], [0.1, 0.1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(recall, precision, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
